{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch 1/150\n",
      "89/89 [==============================] - 1s 1ms/step - loss: 0.0031 - accuracy: 0.9971\n",
      "Epoch 2/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 3/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 4/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 5/150\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 6/150\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 7/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 8/150\n",
      "89/89 [==============================] - 0s 817us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 9/150\n",
      "89/89 [==============================] - 0s 791us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 10/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 11/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 12/150\n",
      "89/89 [==============================] - 0s 860us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 13/150\n",
      "89/89 [==============================] - 0s 803us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 14/150\n",
      "89/89 [==============================] - 0s 833us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 15/150\n",
      "89/89 [==============================] - 0s 821us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 16/150\n",
      "89/89 [==============================] - 0s 822us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 17/150\n",
      "89/89 [==============================] - 0s 983us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 18/150\n",
      "89/89 [==============================] - 0s 825us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 19/150\n",
      "89/89 [==============================] - 0s 891us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 20/150\n",
      "89/89 [==============================] - 0s 832us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 21/150\n",
      "89/89 [==============================] - 0s 819us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 22/150\n",
      "89/89 [==============================] - 0s 836us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 23/150\n",
      "89/89 [==============================] - 0s 813us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 24/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 25/150\n",
      "89/89 [==============================] - 0s 794us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 26/150\n",
      "89/89 [==============================] - 0s 819us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 27/150\n",
      "89/89 [==============================] - 0s 822us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 28/150\n",
      "89/89 [==============================] - 0s 808us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 29/150\n",
      "89/89 [==============================] - 0s 794us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 30/150\n",
      "89/89 [==============================] - 0s 801us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 31/150\n",
      "89/89 [==============================] - 0s 848us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 32/150\n",
      "89/89 [==============================] - 0s 810us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 33/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 34/150\n",
      "89/89 [==============================] - 0s 832us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 35/150\n",
      "89/89 [==============================] - 0s 819us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 36/150\n",
      "89/89 [==============================] - 0s 864us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 37/150\n",
      "89/89 [==============================] - 0s 837us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 38/150\n",
      "89/89 [==============================] - 0s 807us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 39/150\n",
      "89/89 [==============================] - 0s 807us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 40/150\n",
      "89/89 [==============================] - 0s 808us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 41/150\n",
      "89/89 [==============================] - 0s 784us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 42/150\n",
      "89/89 [==============================] - 0s 792us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 43/150\n",
      "89/89 [==============================] - 0s 825us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 44/150\n",
      "89/89 [==============================] - 0s 835us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 45/150\n",
      "89/89 [==============================] - 0s 831us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 46/150\n",
      "89/89 [==============================] - 0s 830us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 47/150\n",
      "89/89 [==============================] - 0s 827us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 48/150\n",
      "89/89 [==============================] - 0s 855us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 49/150\n",
      "89/89 [==============================] - 0s 837us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 50/150\n",
      "89/89 [==============================] - 0s 850us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 51/150\n",
      "89/89 [==============================] - 0s 841us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 52/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 53/150\n",
      "89/89 [==============================] - 0s 837us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 54/150\n",
      "89/89 [==============================] - 0s 806us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 55/150\n",
      "89/89 [==============================] - 0s 819us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 56/150\n",
      "89/89 [==============================] - 0s 834us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 57/150\n",
      "89/89 [==============================] - 0s 769us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 58/150\n",
      "89/89 [==============================] - 0s 795us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 59/150\n",
      "89/89 [==============================] - 0s 799us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 60/150\n",
      "89/89 [==============================] - 0s 788us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 61/150\n",
      "89/89 [==============================] - 0s 793us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 62/150\n",
      "89/89 [==============================] - 0s 816us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 63/150\n",
      "89/89 [==============================] - 0s 855us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 64/150\n",
      "89/89 [==============================] - 0s 804us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 65/150\n",
      "89/89 [==============================] - 0s 891us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 66/150\n",
      "89/89 [==============================] - 0s 808us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 67/150\n",
      "89/89 [==============================] - 0s 806us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 68/150\n",
      "89/89 [==============================] - 0s 839us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 69/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 70/150\n",
      "89/89 [==============================] - 0s 881us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 71/150\n",
      "89/89 [==============================] - 0s 795us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 72/150\n",
      "89/89 [==============================] - 0s 796us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 73/150\n",
      "89/89 [==============================] - 0s 838us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 74/150\n",
      "89/89 [==============================] - 0s 830us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 75/150\n",
      "89/89 [==============================] - 0s 850us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 76/150\n",
      "89/89 [==============================] - 0s 851us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 77/150\n",
      "89/89 [==============================] - 0s 881us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 78/150\n",
      "89/89 [==============================] - 0s 859us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 79/150\n",
      "89/89 [==============================] - 0s 829us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 80/150\n",
      "89/89 [==============================] - 0s 817us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 81/150\n",
      "89/89 [==============================] - 0s 846us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 82/150\n",
      "89/89 [==============================] - 0s 804us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 83/150\n",
      "89/89 [==============================] - 0s 764us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 84/150\n",
      "89/89 [==============================] - 0s 809us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 85/150\n",
      "89/89 [==============================] - 0s 802us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 86/150\n",
      "89/89 [==============================] - 0s 808us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 87/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 88/150\n",
      "89/89 [==============================] - 0s 853us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 89/150\n",
      "89/89 [==============================] - 0s 842us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 90/150\n",
      "89/89 [==============================] - 0s 815us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 91/150\n",
      "89/89 [==============================] - 0s 852us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 92/150\n",
      "89/89 [==============================] - 0s 823us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 93/150\n",
      "89/89 [==============================] - 0s 833us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 94/150\n",
      "89/89 [==============================] - 0s 822us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 95/150\n",
      "89/89 [==============================] - 0s 797us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 96/150\n",
      "89/89 [==============================] - 0s 848us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 97/150\n",
      "89/89 [==============================] - 0s 820us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 98/150\n",
      "89/89 [==============================] - 0s 802us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 99/150\n",
      "89/89 [==============================] - 0s 820us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 100/150\n",
      "89/89 [==============================] - 0s 816us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 101/150\n",
      "89/89 [==============================] - 0s 808us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 102/150\n",
      "89/89 [==============================] - 0s 829us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 103/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 104/150\n",
      "89/89 [==============================] - 0s 871us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 105/150\n",
      "89/89 [==============================] - 0s 765us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 106/150\n",
      "89/89 [==============================] - 0s 795us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 107/150\n",
      "89/89 [==============================] - 0s 791us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 108/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 109/150\n",
      "89/89 [==============================] - 0s 797us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 110/150\n",
      "89/89 [==============================] - 0s 791us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 111/150\n",
      "89/89 [==============================] - 0s 819us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 112/150\n",
      "89/89 [==============================] - 0s 810us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 113/150\n",
      "89/89 [==============================] - 0s 838us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 114/150\n",
      "89/89 [==============================] - 0s 847us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 115/150\n",
      "89/89 [==============================] - 0s 889us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 116/150\n",
      "89/89 [==============================] - 0s 903us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 117/150\n",
      "89/89 [==============================] - 0s 870us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 118/150\n",
      "89/89 [==============================] - 0s 834us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 119/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 120/150\n",
      "89/89 [==============================] - 0s 841us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 121/150\n",
      "89/89 [==============================] - 0s 831us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 122/150\n",
      "89/89 [==============================] - 0s 834us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 123/150\n",
      "89/89 [==============================] - 0s 800us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 124/150\n",
      "89/89 [==============================] - 0s 861us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 125/150\n",
      "89/89 [==============================] - 0s 797us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 126/150\n",
      "89/89 [==============================] - 0s 794us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 127/150\n",
      "89/89 [==============================] - 0s 793us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 128/150\n",
      "89/89 [==============================] - 0s 819us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 129/150\n",
      "89/89 [==============================] - 0s 779us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 130/150\n",
      "89/89 [==============================] - 0s 763us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 131/150\n",
      "89/89 [==============================] - 0s 783us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 132/150\n",
      "89/89 [==============================] - 0s 785us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 133/150\n",
      "89/89 [==============================] - 0s 788us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 134/150\n",
      "89/89 [==============================] - 0s 820us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 135/150\n",
      "89/89 [==============================] - 0s 803us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 136/150\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 137/150\n",
      "89/89 [==============================] - 0s 847us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 138/150\n",
      "89/89 [==============================] - 0s 815us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 139/150\n",
      "89/89 [==============================] - 0s 829us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 140/150\n",
      "89/89 [==============================] - 0s 800us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 141/150\n",
      "89/89 [==============================] - 0s 817us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 142/150\n",
      "89/89 [==============================] - 0s 839us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 143/150\n",
      "89/89 [==============================] - 0s 827us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 144/150\n",
      "89/89 [==============================] - 0s 861us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 145/150\n",
      "89/89 [==============================] - 0s 822us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 146/150\n",
      "89/89 [==============================] - 0s 828us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 147/150\n",
      "89/89 [==============================] - 0s 849us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 148/150\n",
      "89/89 [==============================] - 0s 815us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 149/150\n",
      "89/89 [==============================] - 0s 841us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "Epoch 150/150\n",
      "89/89 [==============================] - 0s 847us/step - loss: 0.0029 - accuracy: 0.9971\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000012A0F9E8310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 998us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "MSE =  8869.874230240457\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset diabetes\n",
    "diabetes = datasets.load_diabetes()\n",
    "x = diabetes.data\n",
    "y = diabetes.target\n",
    "features = diabetes.feature_names\n",
    "n_features = len(features)\n",
    "\n",
    "# Create output variables from original labels. This is required only in multiclass problems.\n",
    "output_y = np_utils.to_categorical(y)   \n",
    "print(output_y)\n",
    "\n",
    "# Define MLP model\n",
    "clf = Sequential()\n",
    "clf.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "clf.add(Dense(10, activation='relu'))\n",
    "clf.add(Dense(1, activation='linear')) # for 2-class problems, use clf.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "clf.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "# Fit model\n",
    "clf.fit(x, output_y, epochs=150, batch_size=5)\n",
    "\n",
    "# Evaluate model\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "\n",
    "mse = 0\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "\n",
    "    # Training phase\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    clf_cv = Sequential()\n",
    "    clf_cv.add(Dense(10, input_dim=n_features, activation='relu'))\n",
    "    clf_cv.add(Dense(10, activation='relu'))\n",
    "    clf_cv.add(Dense(1, activation='linear'))\n",
    "    clf_cv.compile(loss='mean_squared_error', optimizer='adam') \n",
    "    clf_cv.fit(x_train, y_train, epochs=150, batch_size=5, verbose=0)    \n",
    "\n",
    "    # Test phase\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    y_pred = clf_cv.predict(x_test)\n",
    "\n",
    "    mse += ((y_test - y_pred) ** 2).mean()\n",
    "\n",
    "mse /= 5\n",
    "print('MSE = ', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 979us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 980us/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 802us/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 984us/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1000us/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "MSE with batch size 10: 3062.62511431542\n",
      "MSE with batch size 20: 3285.40459720405\n",
      "MSE with batch size 30: 3415.4689369219473\n",
      "MSE with batch size 40: 3642.231151669538\n",
      "MSE with batch size 50: 3950.171627325975\n",
      "MSE with batch size 60: 4746.949564171033\n",
      "MSE with batch size 70: 5154.524091910501\n",
      "MSE with batch size 80: 10963.793460108802\n",
      "MSE with batch size 90: 12427.65420352184\n",
      "MSE with batch size 100: 12260.572598494382\n",
      "MSE with batch size 110: 9836.289122590413\n",
      "MSE with batch size 120: 19723.78964505688\n",
      "MSE with batch size 130: 18510.88796513753\n",
      "MSE with batch size 140: 18760.903174367042\n",
      "MSE with batch size 150: 17019.509795974394\n",
      "MSE with batch size 160: 19915.15601035765\n",
      "MSE with batch size 170: 21892.555503575863\n",
      "MSE with batch size 180: 26133.88833721342\n",
      "MSE with batch size 190: 26590.367216258637\n",
      "MSE with batch size 200: 26037.31554352103\n"
     ]
    }
   ],
   "source": [
    "# Definiendo diferentes tama√±os de lote\n",
    "# Define MLP model\n",
    "clf = Sequential()\n",
    "clf.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "clf.add(Dense(10, activation='relu'))\n",
    "clf.add(Dense(1, activation='linear')) # for 2-class problems, use clf.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "clf.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Set the batch sizes to test\n",
    "batch_sizes = list(range(10, 201, 10))\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "# Perform cross-validation for each batch size\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    mse = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        # Training phase\n",
    "        x_train = x[train_index, :]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        clf_cv = Sequential()\n",
    "        clf_cv.add(Dense(10, input_dim=n_features, activation='relu'))\n",
    "        clf_cv.add(Dense(10, activation='relu'))\n",
    "        clf_cv.add(Dense(1, activation='linear'))\n",
    "        clf_cv.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        clf_cv.fit(x_train, y_train, epochs=150, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Test phase\n",
    "        x_test = x[test_index, :]\n",
    "        y_test = y[test_index]\n",
    "        y_pred = clf_cv.predict(x_test)\n",
    "\n",
    "        mse += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    mse /= 5\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Print MSE scores for each batch size\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "    print(f\"MSE with batch size {batch_size}: {mse_scores[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
